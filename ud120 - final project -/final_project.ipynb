{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F:\\\\ud120\\\\final_project\\\\'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"F:\\\\ud120\\\\final_project\\\\\")\n",
    "path= os.getcwd() + \"\\\\\"\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sys\n",
    "import scipy\n",
    "import matplotlib \n",
    "#import pylab\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import neighbors\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# finction for reading a dictionary as a dataframe\n",
    "def dict_to_dataframe(dictionary):\n",
    "\n",
    "    df = pd.DataFrame.from_dict(dictionary).transpose()\n",
    "    df.apply(partial(pd.to_numeric, errors='ignore'))\n",
    "\n",
    "    df.reset_index(level=0, inplace=True)\n",
    "    columns = list(df.columns)\n",
    "    columns[0] = 'staff_name'\n",
    "    df.columns = columns\n",
    "    \n",
    "    return(df)\n",
    "\n",
    "# function for counting 'NaN' values without replacing\n",
    "def count_nan(column):\n",
    "    k = 0\n",
    "    for value in column:\n",
    "        if value == 'NaN':\n",
    "            k += 1\n",
    "    p = 100.0*k/len(column)\n",
    "    return k, p\n",
    "\n",
    "# function for cleaning 'NaN' values without replacing\n",
    "def column_without_nan(column):\n",
    "    data = []\n",
    "    for value in column:\n",
    "        if value == 'NaN':\n",
    "            continue\n",
    "        data.append(value)\n",
    "    return data\n",
    "\n",
    "# function for cleaning 'NaN' values with replacing\n",
    "def column_with_npnan(column):\n",
    "    data = []\n",
    "    for value in column:\n",
    "        if value == 'NaN':\n",
    "            value = np.nan\n",
    "        data.append(value)\n",
    "    return np.array(data)\n",
    "\n",
    "# function for displaying 3 top values\n",
    "def show_three_top(data, feature):\n",
    "    sorted_list = sorted(column_without_nan(data[feature]), reverse=True)[0:3]\n",
    "    return sorted_list\n",
    "\n",
    "def featureFormat( dictionary, features, remove_NaN=True, \n",
    "                  remove_all_zeroes=True, remove_any_zeroes=False, sort_keys = False):\n",
    "    return_list = []\n",
    "\n",
    "    # Key order - first branch is for Python 3 compatibility on mini-projects,\n",
    "    # second branch is for compatibility on final project.\n",
    "    if isinstance(sort_keys, str):\n",
    "        keys = pickle.load(open(sort_keys, \"rb\"))\n",
    "    elif sort_keys:\n",
    "        keys = sorted(dictionary.keys())\n",
    "    else:\n",
    "        keys = dictionary.keys()\n",
    "\n",
    "    for key in keys:\n",
    "        tmp_list = []\n",
    "        for feature in features:\n",
    "            try:\n",
    "                dictionary[key][feature]\n",
    "            except KeyError:\n",
    "                print (\"error: key \", feature, \" not present\")\n",
    "                return\n",
    "            value = dictionary[key][feature]\n",
    "            if value==\"NaN\" and remove_NaN:\n",
    "                value = 0\n",
    "            tmp_list.append( float(value) )\n",
    "        # Logic for deciding whether or not to add the data point.\n",
    "        append = True\n",
    "        # exclude 'poi' class as criteria.\n",
    "        if features[0] == 'poi':\n",
    "            test_list = tmp_list[1:]\n",
    "        else:\n",
    "            test_list = tmp_list\n",
    "        ### if all features are zero and you want to remove\n",
    "        ### data points that are all zero, do that here\n",
    "        if remove_all_zeroes:\n",
    "            append = False\n",
    "            for item in test_list:\n",
    "                if item != 0 and item != \"NaN\":\n",
    "                    append = True\n",
    "                    break\n",
    "        ### if any features for a given data point are zero\n",
    "        ### and you want to remove data points with any zeroes,\n",
    "        ### handle that here\n",
    "        if remove_any_zeroes:\n",
    "            if 0 in test_list or \"NaN\" in test_list:\n",
    "                append = False\n",
    "        ### Append the data point if flagged for addition.\n",
    "        if append:\n",
    "            return_list.append( np.array(tmp_list) )\n",
    "\n",
    "    return np.array(return_list) \n",
    "\n",
    "def targetFeatureSplit( data ):\n",
    "    target = []\n",
    "    features = []\n",
    "    for item in data:\n",
    "        target.append( item[0] )\n",
    "        features.append( item[1:] )\n",
    "\n",
    "    return target, features\n",
    "\n",
    "PERF_FORMAT_STRING = \"\\\n",
    "\\tAccuracy: {:>0.{display_precision}f}\\tPrecision: {:>0.{display_precision}f}\\t\\\n",
    "Recall: {:>0.{display_precision}f}\\tF1: {:>0.{display_precision}f}\\tF2: {:>0.{display_precision}f}\"\n",
    "RESULTS_FORMAT_STRING = \"\\tTotal predictions: {:4d}\\tTrue positives: {:4d}\\tFalse positives: {:4d}\\\n",
    "\\tFalse negatives: {:4d}\\tTrue negatives: {:4d}\"\n",
    "\n",
    "\n",
    "def test_classifier(clf, dataset, feature_list, folds = 1000):\n",
    "    data = featureFormat(dataset, feature_list, sort_keys = True)\n",
    "    labels, features = targetFeatureSplit(data)\n",
    "    cv = StratifiedShuffleSplit(labels, folds, random_state = 42)\n",
    "    true_negatives = 0\n",
    "    false_negatives = 0\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    for train_idx, test_idx in cv: \n",
    "        features_train = []\n",
    "        features_test  = []\n",
    "        labels_train   = []\n",
    "        labels_test    = []\n",
    "        for ii in train_idx:\n",
    "            features_train.append( features[ii] )\n",
    "            labels_train.append( labels[ii] )\n",
    "        for jj in test_idx:\n",
    "            features_test.append( features[jj] )\n",
    "            labels_test.append( labels[jj] )\n",
    "        \n",
    "        ### fit the classifier using training set, and test on test set\n",
    "        clf.fit(features_train, labels_train)\n",
    "        predictions = clf.predict(features_test)\n",
    "        for prediction, truth in zip(predictions, labels_test):\n",
    "            if prediction == 0 and truth == 0:\n",
    "                true_negatives += 1\n",
    "            elif prediction == 0 and truth == 1:\n",
    "                false_negatives += 1\n",
    "            elif prediction == 1 and truth == 0:\n",
    "                false_positives += 1\n",
    "            elif prediction == 1 and truth == 1:\n",
    "                true_positives += 1\n",
    "            else:\n",
    "                print (\"Warning: Found a predicted label not == 0 or 1.\")\n",
    "                print (\"All predictions should take value 0 or 1.\")\n",
    "                print (\"Evaluating performance for processed predictions:\")\n",
    "                break\n",
    "    try:\n",
    "        total_predictions = true_negatives + false_negatives + false_positives + true_positives\n",
    "        accuracy = 1.0*(true_positives + true_negatives)/total_predictions\n",
    "        precision = 1.0*true_positives/(true_positives+false_positives)\n",
    "        recall = 1.0*true_positives/(true_positives+false_negatives)\n",
    "        f1 = 2.0 * true_positives/(2*true_positives + false_positives+false_negatives)\n",
    "        f2 = (1+2.0*2.0) * precision*recall/(4*precision + recall)\n",
    "        print (clf)\n",
    "        print (PERF_FORMAT_STRING.format(accuracy, precision, recall, f1, f2, display_precision = 5))\n",
    "        print (RESULTS_FORMAT_STRING.format(total_predictions, true_positives, false_positives, false_negatives, true_negatives))\n",
    "        print (\"\")\n",
    "    except:\n",
    "        print (\"Got a divide by zero when trying out:\", clf)\n",
    "        print (\"Precision or recall may be undefined due to a lack of true positive predicitons.\")\n",
    "\n",
    "CLF_PICKLE_FILENAME = path+\"my_classifier.pkl\"\n",
    "DATASET_PICKLE_FILENAME = path+\"my_dataset.pkl\"\n",
    "FEATURE_LIST_FILENAME = path+\"my_feature_list.pkl\"\n",
    "        \n",
    "def dump_classifier_and_data(clf, dataset, feature_list):\n",
    "    with open(CLF_PICKLE_FILENAME, \"wb\") as clf_outfile:\n",
    "        pickle.dump(clf, clf_outfile)\n",
    "    with open(DATASET_PICKLE_FILENAME, \"wb\") as dataset_outfile:\n",
    "        pickle.dump(dataset, dataset_outfile)\n",
    "    with open(FEATURE_LIST_FILENAME, \"wb\") as featurelist_outfile:\n",
    "        pickle.dump(feature_list, featurelist_outfile)\n",
    "\n",
    "def load_classifier_and_data():\n",
    "    with open(CLF_PICKLE_FILENAME, \"rb\") as clf_infile:\n",
    "        clf = pickle.load(clf_infile)\n",
    "    with open(DATASET_PICKLE_FILENAME, \"rb\") as dataset_infile:\n",
    "        dataset = pickle.load(dataset_infile)\n",
    "    with open(FEATURE_LIST_FILENAME, \"rb\") as featurelist_infile:\n",
    "        feature_list = pickle.load(featurelist_infile)\n",
    "    return clf, dataset, feature_list\n",
    "\n",
    "def main():\n",
    "    ### load up student's classifier, dataset, and feature_list\n",
    "    clf, dataset, feature_list = load_classifier_and_data()\n",
    "    ### Run testing script\n",
    "    test_classifier(clf, dataset, feature_list)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19, 1, 145)\n",
      "[[ 0.51765448  0.14272044  0.14272044 ...,  0.14272044  0.14272044\n",
      "   0.14272044]\n",
      " [ 0.44582382  0.0266425   0.13384545 ...,  0.13384545  0.13384545\n",
      "   0.13384545]\n",
      " [ 0.12082906  0.83717943  0.99878095 ...,  0.83717943  0.83717943\n",
      "   0.83717943]\n",
      " ..., \n",
      " [ 0.15206186  0.04157082  0.00118417 ...,  0.04157082  0.04157082\n",
      "   0.04157082]\n",
      " [ 0.10673235  0.06770535  0.         ...,  0.06770535  0.06770535\n",
      "   0.06770535]\n",
      " [ 0.2545751   0.21280397  0.08389201 ...,  0.21280397  0.21280397\n",
      "   0.21280397]]\n"
     ]
    }
   ],
   "source": [
    "enron_data = pickle.load(open(\"final_project_dataset.pkl\", \"rb\"))\n",
    "\n",
    "# Get names and count the persons of interest\n",
    "k_poi=0\n",
    "poi = []\n",
    "for i in range(len(enron_data.keys())):\n",
    "    person = list(enron_data.keys())[i]\n",
    "    if enron_data[person]['poi'] == True:\n",
    "        k_poi += 1\n",
    "        poi.append(person)\n",
    "\n",
    "# Construct the dataframe from the dictionary\n",
    "enron_df = dict_to_dataframe(enron_data)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create columns 'staff_name', 'salary' without NaN\n",
    "salary_name = enron_df[['staff_name', 'salary']]\n",
    "salary_name = salary_name[salary_name['salary'] != 'NaN']\n",
    "\n",
    "# Find the name of the outlier\n",
    "salary_name['staff_name'][salary_name['salary'].idxmax()]\n",
    "\n",
    "# Delete the record 'TOTAL'\n",
    "del enron_data['TOTAL']\n",
    "enron_df = enron_df[enron_df['staff_name'] != 'TOTAL']\n",
    "\n",
    "# Create columns 'staff_name', 'salary' without NaN after deleting the outlier\n",
    "salary_name = enron_df[['staff_name', 'salary']]\n",
    "salary_name = salary_name[salary_name['salary'] != 'NaN']\n",
    "\n",
    "# Create dataframe with replaced NaN by zero\n",
    "enron_df1 = pd.DataFrame(enron_df)\n",
    "enron_df1 = enron_df1.convert_objects(convert_numeric=True)\n",
    "enron_df1 = enron_df1.fillna(0)\n",
    "# Create dataframe to check total payments\n",
    "enron_df2 = pd.DataFrame()\n",
    "enron_df2['staff_name'] = enron_df1['staff_name']\n",
    "enron_df2['total_check'] = enron_df1['bonus'] + enron_df1['director_fees'] + enron_df1['deferral_payments'] + \\\n",
    "    enron_df1['deferred_income'] + enron_df1['loan_advances'] + enron_df1['long_term_incentive'] + \\\n",
    "    enron_df1['expenses'] + enron_df1['other'] + enron_df1['salary']\n",
    "enron_df2['total_payments'] = enron_df1['total_payments']\n",
    "enron_df2['stock_check'] = (enron_df1['restricted_stock'] + enron_df1['exercised_stock_options'] + \\\n",
    "                            enron_df1['restricted_stock_deferred'])\n",
    "enron_df2['total_stock_value'] = enron_df1['total_stock_value']\n",
    "enron_df2['same_total'] = (enron_df2['total_check'] == enron_df2['total_payments'])\n",
    "enron_df2['same_stock'] = (enron_df2['stock_check'] == enron_df2['total_stock_value'])\n",
    "enron_df2['poi'] = enron_df1['poi']\n",
    "\n",
    "# Replacing values in 2 rows in the dictionary\n",
    "enron_data['BELFER ROBERT']['deferred_income'] = -102500\n",
    "enron_data['BELFER ROBERT']['deferral_payments'] = 'NaN'\n",
    "enron_data['BELFER ROBERT']['director_fees'] = 102500\n",
    "enron_data['BELFER ROBERT']['expenses'] = 3285\n",
    "enron_data['BELFER ROBERT']['total_payments'] = 3285\n",
    "enron_data['BELFER ROBERT']['exercised_stock_options'] = 'NaN'\n",
    "enron_data['BELFER ROBERT']['restricted_stock'] = 44093\n",
    "enron_data['BELFER ROBERT']['restricted_stock_deferred'] = -44093\n",
    "enron_data['BELFER ROBERT']['total_stock_value'] = 'NaN'\n",
    "\n",
    "enron_data['BHATNAGAR SANJAY']['director_fees'] = 'NaN'\n",
    "enron_data['BHATNAGAR SANJAY']['expenses'] = 137864\n",
    "enron_data['BHATNAGAR SANJAY']['other'] = 'NaN'\n",
    "enron_data['BHATNAGAR SANJAY']['total_payments'] = 137864\n",
    "enron_data['BHATNAGAR SANJAY']['exercised_stock_options'] = 15456290\n",
    "enron_data['BHATNAGAR SANJAY']['restricted_stock'] = 2604490\n",
    "enron_data['BHATNAGAR SANJAY']['restricted_stock_deferred'] = -2604490\n",
    "enron_data['BHATNAGAR SANJAY']['total_stock_value'] = 15456290\n",
    "# Replacing values in 2 rows in the dataframe\n",
    "enron_df = dict_to_dataframe(enron_data)\n",
    "\n",
    "# Check replacing\n",
    "enron_df[(enron_df['staff_name'] == 'BHATNAGAR SANJAY') | (enron_df['staff_name'] == 'BELFER ROBERT')].T\n",
    "\n",
    "# Create a list of finance features\n",
    "finance_feature_list = ['bonus', 'deferral_payments', 'deferred_income', 'director_fees', 'exercised_stock_options',\n",
    "                        'expenses', 'loan_advances', 'long_term_incentive', 'other', 'restricted_stock', \n",
    "                        'restricted_stock_deferred', 'salary', 'total_payments', 'total_stock_value']\n",
    "\n",
    "# Create a list of email features\n",
    "email_feature_list = ['to_messages', 'from_poi_to_this_person', 'from_messages', \n",
    "                      'from_this_person_to_poi', 'shared_receipt_with_poi']\n",
    "\n",
    "# Find 1 outlier for each email feature\n",
    "email_outliers = enron_df[(enron_df['staff_name'] == 'KAMINSKI WINCENTY J') | \n",
    "                          (enron_df['staff_name'] == 'SHAPIRO RICHARD S') | \n",
    "                          (enron_df['staff_name'] == 'DELAINEY DAVID W') | \n",
    "                          (enron_df['staff_name'] == 'LAVORATO JOHN J') |\n",
    "                          (enron_df['staff_name'] == 'BELDEN TIMOTHY N') ] \\\n",
    "[['staff_name', 'to_messages', 'from_messages', \n",
    "  'from_poi_to_this_person', 'from_this_person_to_poi', 'shared_receipt_with_poi']]\n",
    "\n",
    "email_outliers_list = ['KAMINSKI WINCENTY J', 'SHAPIRO RICHARD S', 'DELAINEY DAVID W', \n",
    "                       'LAVORATO JOHN J', 'BELDEN TIMOTHY N']\n",
    "email_outliers\n",
    "\n",
    "# Replace string NaN by np.nan\n",
    "enron_df_np = enron_df.apply(column_with_npnan)\n",
    "\n",
    "# Setup Imputer\n",
    "imp = Imputer(missing_values='NaN', strategy='mean', axis=1)\n",
    "# Setup Scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Setup feature list without email address\n",
    "feature_list0 = ['bonus', 'deferral_payments', 'deferred_income', 'director_fees', 'expenses', \n",
    "                'exercised_stock_options', 'loan_advances', 'long_term_incentive', 'other', 'restricted_stock', \n",
    "                'restricted_stock_deferred', 'salary', 'total_payments', 'total_stock_value',\n",
    "                'to_messages', 'from_poi_to_this_person', 'from_messages', 'from_this_person_to_poi', \n",
    "                 'shared_receipt_with_poi']\n",
    "\n",
    "# Setup variable for features after Imputer and Scaler\n",
    "feature_imp = [[0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]\n",
    "feature_imp_scaled = [[0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]\n",
    "\n",
    "\n",
    "# Transform data for finance features by Imputer\n",
    "for i in range(len(feature_list0)):\n",
    "    element = feature_list0[i]\n",
    "    #print(element)\n",
    "    imp.fit([enron_df_np[element]])\n",
    "    #print(np.array([enron_df_np[element]]).shape)\n",
    "    feature_imp[i] = imp.transform([enron_df_np[element]])\n",
    "    #print(feature_imp[i].shape)\n",
    "    #print(feature_imp[i][0])\n",
    "    feature_imp[i] = feature_imp[i][0]\n",
    "    #print(np.array([feature_imp[i]]).shape)\n",
    "    #print(scaler.fit_transform([feature_imp[i]]))\n",
    "    feature_imp_scaled[i] = scaler.fit_transform(feature_imp[i].reshape(-1,1))\n",
    "    feature_imp_scaled[i]=np.reshape(feature_imp_scaled[i],(1,145))\n",
    "    #print( feature_imp_scaled[i])\n",
    "\n",
    "print(np.array(feature_imp_scaled).shape)\n",
    "feature_imp_scaled= np.reshape(feature_imp_scaled, (1,19,145))\n",
    "feature_imp_scaled=feature_imp_scaled[0]\n",
    "print(feature_imp_scaled)\n",
    "enron_df_imp_scaled = pd.DataFrame(feature_imp_scaled)\n",
    "enron_df_imp_scaled.index = feature_list0\n",
    "\n",
    "#Transform the dataframe\n",
    "enron_df_imp_scaled = enron_df_imp_scaled.T\n",
    "\n",
    "# Complete a scaled dataframe\n",
    "df1 = enron_df['staff_name']\n",
    "df2 = enron_df['email_address']\n",
    "df3 = enron_df['poi']\n",
    "scaled_enron_df = pd.concat([enron_df_imp_scaled, df1, df2, df3], axis=1)\n",
    "scaled_enron_df.head().T\n",
    "\n",
    "scaled_enron_data = scaled_enron_df.to_dict(orient=\"index\")\n",
    "\n",
    "correlation_enron_df = pd.DataFrame(scaled_enron_df)\n",
    "\n",
    "\n",
    "features_list01 = ['poi','salary', 'bonus', 'exercised_stock_options', 'deferred_income']\n",
    "features_list02 = ['poi','salary', 'bonus', 'exercised_stock_options', 'deferred_income', \n",
    "                   'expenses', 'long_term_incentive', 'restricted_stock']\n",
    "features_list03 = ['poi', 'salary', 'bonus', 'exercised_stock_options', 'deferred_income', \n",
    "                   'long_term_incentive', 'expenses']\n",
    "features_list04 = ['poi','from_messages', 'from_this_person_to_poi', 'shared_receipt_with_poi']\n",
    "features_list05 = ['poi','salary', 'bonus', 'exercised_stock_options', 'deferred_income', \n",
    "                  'from_poi_to_this_person', 'from_this_person_to_poi', 'shared_receipt_with_poi']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=4, p=2,\n",
      "           weights='distance')\n",
      "\tAccuracy: 0.88107\tPrecision: 0.64231\tRecall: 0.37800\tF1: 0.47592\tF2: 0.41190\n",
      "\tTotal predictions: 14000\tTrue positives:  756\tFalse positives:  421\tFalse negatives: 1244\tTrue negatives: 11579\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "my_dataset = enron_data\n",
    "\n",
    "'''\n",
    "clf01 =  DecisionTreeClassifier(max_depth=1)\n",
    "clf02 =  AdaBoostClassifier()\n",
    "clf03 =  RandomForestClassifier(min_samples_split=50)\n",
    "clf04 =  GaussianNB()\n",
    "clf05 =  neighbors.KNeighborsClassifier(n_neighbors=4, weights='distance')\n",
    "clf06 =  QuadraticDiscriminantAnalysis()\n",
    "clf07 =  KMeans(n_clusters=2)\n",
    "clf08 =  LogisticRegression()\n",
    "'''\n",
    "\n",
    "data01 = featureFormat(my_dataset, features_list01, sort_keys = True)\n",
    "labels01, features01 = targetFeatureSplit(data01)\n",
    "\n",
    "features_train01, features_test01, labels_train01, labels_test01 = \\\n",
    "    train_test_split(features01, labels01, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "# Create a dataframe for engineering\n",
    "engineer_enron_df = pd.DataFrame(scaled_enron_df)\n",
    "\n",
    "# Create new features\n",
    "engineer_enron_df['coefficient_bonus_salary'] = 0.0\n",
    "engineer_enron_df['coefficient_from_poi_all'] = 0.0\n",
    "engineer_enron_df['coefficient_to_poi_all'] = 0.0\n",
    "engineer_enron_df['coefficient_income_total'] = 0.0\n",
    "\n",
    "for i in range(len(scaled_enron_df['salary'])):\n",
    "    if scaled_enron_df['salary'][i] > 0:\n",
    "        engineer_enron_df['coefficient_bonus_salary'][i] = \\\n",
    "        1.0 * scaled_enron_df['bonus'][i] / scaled_enron_df['salary'][i]\n",
    "for i in range(len(scaled_enron_df['to_messages'])):\n",
    "    if scaled_enron_df['to_messages'][i] > 0:\n",
    "        engineer_enron_df['coefficient_from_poi_all'][i] = \\\n",
    "        1.0 * scaled_enron_df['from_poi_to_this_person'][i] / scaled_enron_df['to_messages'][i]\n",
    "for i in range(len(scaled_enron_df['from_messages'])):\n",
    "    if scaled_enron_df['from_messages'][i] > 0:\n",
    "        engineer_enron_df['coefficient_to_poi_all'][i] = \\\n",
    "        1.0 * (scaled_enron_df['from_this_person_to_poi'][i] + scaled_enron_df['shared_receipt_with_poi'][i]) \\\n",
    "        / scaled_enron_df['from_messages'][i]\n",
    "for i in range(len(scaled_enron_df['total_payments'])):\n",
    "    if scaled_enron_df['total_payments'][i] > 0:\n",
    "        engineer_enron_df['coefficient_income_total'][i] = \\\n",
    "        1.0 * scaled_enron_df['deferred_income'][i] / scaled_enron_df['total_payments'][i]\n",
    "\n",
    "\n",
    "# Reading the dataframe into a dictionary\n",
    "engineer_enron_data = engineer_enron_df.to_dict(orient=\"index\")\n",
    "\n",
    "my_dataset3 = engineer_enron_data\n",
    "\n",
    "features_list06 = ['poi', 'coefficient_bonus_salary', 'coefficient_income_total',\n",
    "                   'coefficient_from_poi_all', 'coefficient_to_poi_all',\n",
    "                   'exercised_stock_options']\n",
    "\n",
    "data36 = featureFormat(my_dataset3, features_list06, sort_keys = True)\n",
    "labels36, features36 = targetFeatureSplit(data36)\n",
    "\n",
    "features_train36, features_test36, labels_train36, labels_test36 = \\\n",
    "    train_test_split(features36, labels36, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "clf21 =  DecisionTreeClassifier(min_samples_split=15, max_depth=7)\n",
    "dump_classifier_and_data(clf21, my_dataset3, features_list06)\n",
    "load_classifier_and_data()\n",
    "\n",
    "'''\n",
    "### load up student's classifier, dataset, and feature_list\n",
    "clf, dataset, feature_list = load_classifier_and_data()\n",
    "### Run testing script\n",
    "test_classifier(clf, dataset, feature_list)\n",
    "'''\n",
    "\n",
    "clf45 =  neighbors.KNeighborsClassifier(n_neighbors=4, weights='distance')\n",
    "dump_classifier_and_data(clf45, my_dataset, features_list01)\n",
    "load_classifier_and_data()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
